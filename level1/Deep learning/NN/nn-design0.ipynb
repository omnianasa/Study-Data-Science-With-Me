{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network Design\n\nEach part in the neural network such as layers, neurons, activations, and parameters plays a special role in how the network learns and understands data. The goal of this notebook is to study how every design choice affects learning and performance. By changing the number of layers, the size of filters, learning rate, and other parameters, we can see how the network behaves, improves, or fails. Understanding these details helps us build better and more efficient models that can solve real problems in vision, language, and creativity.\n\n### What We will cover\n\n#### 1. Architecture Design\n\n- Number of layers (depth): How many layers (hidden, convolutional, etc.) the network has.\n\n- Layer types: Dense (Fully Connected), Convolutional, Recurrent, Attention, Residual, etc.\n\n- Layer order and connections: Sequential vs. skip connections (e.g. ResNet, U-Net).\n\n- Width of layers: Number of neurons or filters per layer.\n\n- Bottlenecks / latent dimensions: How compressed the representation becomes (important in VAEs and autoencoders).\n\n#### 2. Neuron-Level Parameters\n\n- Activation functions: ReLU, LeakyReLU, Sigmoid, Tanh, GELU, Swish.\n\n- Normalization: BatchNorm, LayerNorm, GroupNorm; helps stabilize learning.\n\n- Dropout rate: Percentage of neurons randomly dropped to prevent overfitting.\n\n#### 3. Training Parameters (Optimization)\n\n- Learning rate: The most important hyperparameter controlling how fast the model learns.\n\n- Optimizer type: SGD, Adam, RMSProp, AdamW.\n\n- Weight initialization: Xavier, He, Normal, Uniform; affects early learning stability.\n\n- Loss function: MSE, CrossEntropy, KL Divergence, etc., depending on the task.\n\n- Batch size: Number of samples per gradient update.\n\n- Number of epochs: How many times the full dataset passes through the model.\n\n#### 4. Data-Related Parameters\n\n- Input size / image resolution: Affects both accuracy and computation.\n\n- Data augmentation: Flips, rotations, noise, color jitter; improves generalization.\n\n- Normalization / scaling: Preprocessing to balance data distribution.\n\n#### 5. Regularization and Generalization\n\n- L1 / L2 weight decay: Penalizes large weights to prevent overfitting.\n\n- Early stopping: Stops training when validation loss stops improving.\n\n- Dropout: Revisited here as a key technique for generalization control.\n\n#### 6. Advanced Architectural Controls\n\n- Skip connections: Allow gradients to flow through long networks (ResNet, U-Net).\n\n- Attention mechanisms: Help focus on important features (used in Transformers).\n\n- Multi-branch architectures: Parallel paths combining different feature maps.\n\n- Latent space structure: In VAEs or GANs, defines how data is represented internally.\n\n#### 7. Evaluation and Metrics\n\n- Training vs. validation loss: Used for detecting overfitting.\n\n- Accuracy / F1 / Precision / Recall: Common metrics for classification tasks.\n\n- PSNR / SSIM / FID: Metrics for generative or image-based models.\n\n- Learning curves: Visualize training and validation behavior over time.","metadata":{}},{"cell_type":"markdown","source":"# Day 0","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\n\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:43:13.178601Z","iopub.execute_input":"2025-10-17T19:43:13.179599Z","iopub.status.idle":"2025-10-17T19:43:22.489583Z","shell.execute_reply.started":"2025-10-17T19:43:13.179555Z","shell.execute_reply":"2025-10-17T19:43:22.488577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom PIL import Image\n\n\nclass CustomDataset(Dataset):\n    \"\"\"\n    A complete dataset class that:\n      - Handles both torchvision and local folder datasets.\n      - Applies correct transformations and augmentations.\n      - Splits data into train/val/test.\n      - Creates DataLoaders for each split.\n\n    Designed for MNIST and general image folders.\n    \"\"\"\n\n    def __init__(\n        self,\n        root,\n        dataset_name=None,\n        use_torchvision=False,\n        img_size=128,\n        batch_size=8,\n        train_ratio=0.7,\n        val_ratio=0.15,\n        transform=None\n    ):\n        \"\"\"\n        Initialize dataset class parameters.\n\n        Parameters\n        ----------\n        root : str\n            Path to dataset folder or where torchvision downloads data.\n        dataset_name : str\n            Dataset name (\"MNIST\", etc.)\n        use_torchvision : bool\n            Whether to use a built-in dataset (True) or a folder (False).\n        img_size : int\n            Resize images to this size.\n        batch_size : int\n            Number of samples per batch.\n        train_ratio : float\n            Proportion of data for training.\n        val_ratio : float\n            Proportion of data for validation.\n        transform : torchvision.transforms.Compose or None\n            Custom user-defined transform.\n        \"\"\"\n\n        super().__init__()\n        self.root = root\n        self.dataset_name = dataset_name\n        self.use_torchvision = use_torchvision\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.train_ratio = train_ratio\n        self.val_ratio = val_ratio\n        self.transform = transform\n\n        # Determine color channels automatically\n        if dataset_name == \"MNIST\":\n            mean, std = [0.5], [0.5]  # Single channel (grayscale)\n        else:\n            mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]  # RGB\n\n        # Training transformations — with data augmentation\n        self.train_transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n        # Validation/Test transformations — without augmentation\n        self.test_transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n    def __len__(self):\n        \"\"\"\n        Return the number of images in a local folder dataset.\n        Not used for torchvision datasets.\n        \"\"\"\n        if not self.use_torchvision:\n            return len(os.listdir(self.root))\n        else:\n            raise NotImplementedError(\"Torchvision datasets handle __len__ internally.\")\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve a single image (and its label if available).\n        Only used for folder datasets (not torchvision).\n        \"\"\"\n        img_path = os.path.join(self.root, os.listdir(self.root)[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = self.test_transform(image)\n        return image\n\n    def setup(self):\n        \"\"\"\n        Prepare datasets for training, validation, and testing.\n\n        Includes:\n        - Downloading or loading data.\n        - Splitting datasets.\n        - Assigning transforms.\n\n        Splitting logic:\n        - Training: for model learning.\n        - Validation: for hyperparameter tuning.\n        - Test: for final evaluation.\n        \"\"\"\n        if self.use_torchvision:\n            if self.dataset_name == \"MNIST\":\n                # Load MNIST from torchvision\n                full_dataset = datasets.MNIST(\n                    root=self.root,\n                    train=True,\n                    download=True,\n                    transform=self.train_transform\n                )\n                test_dataset = datasets.MNIST(\n                    root=self.root,\n                    train=False,\n                    download=True,\n                    transform=self.test_transform\n                )\n\n                total_len = len(full_dataset)\n                train_len = int(self.train_ratio * total_len)\n                val_len = total_len - train_len\n\n                # Randomly split training into train and val\n                self.train_ds, self.val_ds = random_split(full_dataset, [train_len, val_len])\n                self.test_ds = test_dataset\n            else:\n                raise ValueError(\"Currently only MNIST is supported for torchvision datasets.\")\n        else:\n            # Custom local folder dataset\n            full_dataset = CustomDataset(root=self.root, transform=self.test_transform)\n\n            total_len = len(full_dataset)\n            train_len = int(self.train_ratio * total_len)\n            val_len = int(self.val_ratio * total_len)\n            test_len = total_len - train_len - val_len\n\n            # Split into train/val/test\n            self.train_ds, self.val_ds, self.test_ds = random_split(\n                full_dataset, [train_len, val_len, test_len]\n            )\n\n            # Assign transforms\n            self.train_ds.dataset.transform = self.train_transform\n            self.val_ds.dataset.transform = self.test_transform\n            self.test_ds.dataset.transform = self.test_transform\n\n    def train_loader(self):\n        \"\"\"\n        Create DataLoader for the training dataset.\n        Shuffle=True for better generalization.\n        \"\"\"\n        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n\n    def val_loader(self):\n        \"\"\"\n        Create DataLoader for validation dataset.\n        Shuffle=False for stable evaluation.\n        \"\"\"\n        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False)\n\n    def test_loader(self):\n        \"\"\"\n        Create DataLoader for test dataset.\n        Used for final evaluation.\n        \"\"\"\n        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:43:22.491147Z","iopub.execute_input":"2025-10-17T19:43:22.491683Z","iopub.status.idle":"2025-10-17T19:43:22.509928Z","shell.execute_reply.started":"2025-10-17T19:43:22.491655Z","shell.execute_reply":"2025-10-17T19:43:22.508719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_module = CustomDataset(\n    root=\"data\",\n    dataset_name=\"MNIST\",\n    use_torchvision=True,\n    img_size=28,\n    batch_size=64\n)\n\ndata_module.setup()\n\ntrain_loader = data_module.train_loader()\nval_loader = data_module.val_loader()\ntest_loader = data_module.test_loader()\n\nfor imgs, labels in train_loader:\n    print(imgs.shape, labels.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:43:22.510723Z","iopub.execute_input":"2025-10-17T19:43:22.511021Z","iopub.status.idle":"2025-10-17T19:43:26.634952Z","shell.execute_reply.started":"2025-10-17T19:43:22.510997Z","shell.execute_reply":"2025-10-17T19:43:26.633917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Simple Neural Network","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ===============================================================\n# Fully Connected Neural Network for MNIST Classification\n# ===============================================================\n# Purpose:\n#   - Takes a batch of MNIST images (1x28x28)\n#   - Passes them through a series of linear transformations + ReLU activations\n#   - Outputs 10 values (logits), one per class (digits 0–9)\n#\n# This is a simple \"Multi-Layer Perceptron\" (MLP), also called a Feedforward Neural Network.\n# ===============================================================\n\nclass NeuralNetwork(nn.Module):\n    \"\"\"\n    This class defines the structure and computation of our neural network.\n\n    The model follows this pipeline:\n        Input: [batch_size, 1, 28, 28]\n        ↓\n        Flatten → [batch_size, 784]\n        ↓\n        Linear(784 → 512) + ReLU\n        ↓\n        Linear(512 → 512) + ReLU\n        ↓\n        Linear(512 → 10)\n        ↓\n        Output logits: [batch_size, 10]\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # ----------------------------------------------------------\n        # 1. Flatten Layer\n        # ----------------------------------------------------------\n        # Converts each image (1 channel, 28x28 pixels) into a 1D tensor of 784 elements.\n        # Neural networks need a vector as input for fully connected (Linear) layers.\n        # No weights or biases here — it's a simple reshape operation.\n        #\n        # Input shape:  [batch_size, 1, 28, 28]\n        # Output shape: [batch_size, 784]\n        self.flatten = nn.Flatten()\n\n        # ----------------------------------------------------------\n        # 2. Define the Linear + ReLU layers using nn.Sequential\n        # ----------------------------------------------------------\n        self.linear_relu_stack = nn.Sequential(\n            # Linear layer #1\n            # Learns weights of shape [512, 784] and biases of shape [512].\n            # Each output neuron “sees” *all 784 pixels* at once.\n            nn.Linear(28 * 28, 512),\n\n            # ReLU activation\n            # Applies f(x) = max(0, x)\n            # Keeps positive values, zeros out negative ones.\n            nn.ReLU(),\n\n            # Linear layer #2\n            # Learns weights of shape [512, 512] and biases of shape [512].\n            # Each neuron “sees” all 512 outputs from the previous layer.\n            nn.Linear(512, 512),\n\n            # ReLU activation again\n            nn.ReLU(),\n\n            # Linear layer #3 (Output layer)\n            # Learns weights of shape [10, 512] and biases of shape [10].\n            # Each neuron corresponds to one class (digits 0–9).\n            nn.Linear(512, 10)\n        )\n\n    # ----------------------------------------------------------\n    # 3. Forward Pass\n    # ----------------------------------------------------------\n    def forward(self, x):\n        \"\"\"\n        Defines how data moves through the network step by step.\n\n        Args:\n            x: Tensor of shape [batch_size, 1, 28, 28]\n               (A batch of grayscale MNIST images)\n\n        Returns:\n            logits: Tensor of shape [batch_size, 10]\n                    Raw, unnormalized class scores.\n        \"\"\"\n\n        # Step 1: Flatten the image\n        # Before: x.shape = [batch_size, 1, 28, 28]\n        # After:  x.shape = [batch_size, 784]\n        x = self.flatten(x)\n\n        # Step 2: First Linear Layer (784 → 512)\n        # Each output neuron has 784 weights and 1 bias.\n        # Mathematically: y = x @ W^T + b\n        # W: [512, 784], b: [512]\n        # Input shape:  [batch_size, 784]\n        # Output shape: [batch_size, 512]\n        x = self.linear_relu_stack[0](x)\n        x = self.linear_relu_stack[1](x)  # ReLU activation\n\n        # Step 3: Second Linear Layer (512 → 512)\n        # Each of 512 neurons \"looks\" at all 512 values from the previous layer.\n        # W: [512, 512], b: [512]\n        # Input shape:  [batch_size, 512]\n        # Output shape: [batch_size, 512]\n        x = self.linear_relu_stack[2](x)\n        x = self.linear_relu_stack[3](x)  # ReLU activation\n\n        # Step 4: Output Linear Layer (512 → 10)\n        # W: [10, 512], b: [10]\n        # Produces 10 outputs per sample — one per digit class.\n        # Input shape:  [batch_size, 512]\n        # Output shape: [batch_size, 10]\n        logits = self.linear_relu_stack[4](x)\n\n        # These logits are *not probabilities* yet.\n        # Later, CrossEntropyLoss applies Softmax internally.\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:51:15.023742Z","iopub.execute_input":"2025-10-17T19:51:15.024029Z","iopub.status.idle":"2025-10-17T19:51:15.034414Z","shell.execute_reply.started":"2025-10-17T19:51:15.024010Z","shell.execute_reply":"2025-10-17T19:51:15.033073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\nmodel = NeuralNetwork().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:16:20.611159Z","iopub.execute_input":"2025-10-17T20:16:20.611524Z","iopub.status.idle":"2025-10-17T20:16:20.627687Z","shell.execute_reply.started":"2025-10-17T20:16:20.611500Z","shell.execute_reply":"2025-10-17T20:16:20.626400Z"}},"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(f\"Model structure: {model}\\n\\n\")\n\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:16:23.586238Z","iopub.execute_input":"2025-10-17T20:16:23.586918Z","iopub.status.idle":"2025-10-17T20:16:23.597032Z","shell.execute_reply.started":"2025-10-17T20:16:23.586892Z","shell.execute_reply":"2025-10-17T20:16:23.595862Z"}},"outputs":[{"name":"stdout","text":"Model structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0149, -0.0226, -0.0097,  ..., -0.0318, -0.0063, -0.0088],\n        [-0.0318,  0.0300, -0.0080,  ..., -0.0068, -0.0106,  0.0351]],\n       grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0252,  0.0305], grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0292, -0.0166, -0.0224,  ..., -0.0140, -0.0096, -0.0180],\n        [ 0.0011,  0.0438, -0.0197,  ...,  0.0135,  0.0252, -0.0388]],\n       grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0363, -0.0333], grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0284,  0.0345,  0.0140,  ...,  0.0410, -0.0051, -0.0142],\n        [ 0.0005, -0.0135, -0.0187,  ..., -0.0062,  0.0218, -0.0310]],\n       grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0305, 0.0090], grad_fn=<SliceBackward0>) \n\n","output_type":"stream"}],"execution_count":20}]}